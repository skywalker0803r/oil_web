{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_model-V3(duel).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTIlE23rfWAn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch import tensor\n",
        "from torch.nn import Linear,ReLU,Sigmoid\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import r2_score,mean_squared_error\n",
        "from math import sqrt\n",
        "import warnings;warnings.simplefilter('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUQT_bCRfYyA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "0843ef9e-5a03-42de-bed4-d4c07007f2c1"
      },
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/台塑輕油案子/data/phase1/train_4521.csv',index_col=0)\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>T10</th>\n",
              "      <th>T50</th>\n",
              "      <th>T90</th>\n",
              "      <th>N+A</th>\n",
              "      <th>C5NP</th>\n",
              "      <th>C5IP</th>\n",
              "      <th>C5N</th>\n",
              "      <th>C6NP</th>\n",
              "      <th>C6IP</th>\n",
              "      <th>C6N</th>\n",
              "      <th>C6A</th>\n",
              "      <th>C7NP</th>\n",
              "      <th>C7IP</th>\n",
              "      <th>C7N</th>\n",
              "      <th>C7A</th>\n",
              "      <th>C8NP</th>\n",
              "      <th>C8IP</th>\n",
              "      <th>C8N</th>\n",
              "      <th>C8A</th>\n",
              "      <th>C9NP</th>\n",
              "      <th>C9IP</th>\n",
              "      <th>C9N</th>\n",
              "      <th>C9A</th>\n",
              "      <th>C10NP</th>\n",
              "      <th>C10IP</th>\n",
              "      <th>C10N</th>\n",
              "      <th>C10A</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100.5</td>\n",
              "      <td>119.2</td>\n",
              "      <td>146.5</td>\n",
              "      <td>31.978</td>\n",
              "      <td>0.272</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.059</td>\n",
              "      <td>5.293</td>\n",
              "      <td>2.570</td>\n",
              "      <td>2.819</td>\n",
              "      <td>0.494</td>\n",
              "      <td>10.395</td>\n",
              "      <td>8.070</td>\n",
              "      <td>6.411</td>\n",
              "      <td>2.917</td>\n",
              "      <td>9.138</td>\n",
              "      <td>9.649</td>\n",
              "      <td>4.810</td>\n",
              "      <td>5.373</td>\n",
              "      <td>6.405</td>\n",
              "      <td>9.759</td>\n",
              "      <td>4.590</td>\n",
              "      <td>3.661</td>\n",
              "      <td>0.875</td>\n",
              "      <td>5.257</td>\n",
              "      <td>0.525</td>\n",
              "      <td>0.319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>99.6</td>\n",
              "      <td>117.9</td>\n",
              "      <td>145.5</td>\n",
              "      <td>31.568</td>\n",
              "      <td>0.297</td>\n",
              "      <td>0.151</td>\n",
              "      <td>0.062</td>\n",
              "      <td>5.089</td>\n",
              "      <td>2.531</td>\n",
              "      <td>2.804</td>\n",
              "      <td>0.499</td>\n",
              "      <td>10.074</td>\n",
              "      <td>7.958</td>\n",
              "      <td>6.395</td>\n",
              "      <td>2.894</td>\n",
              "      <td>8.970</td>\n",
              "      <td>9.548</td>\n",
              "      <td>4.753</td>\n",
              "      <td>5.443</td>\n",
              "      <td>6.324</td>\n",
              "      <td>9.899</td>\n",
              "      <td>4.301</td>\n",
              "      <td>2.995</td>\n",
              "      <td>0.881</td>\n",
              "      <td>5.591</td>\n",
              "      <td>1.119</td>\n",
              "      <td>0.303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100.0</td>\n",
              "      <td>118.8</td>\n",
              "      <td>145.6</td>\n",
              "      <td>31.344</td>\n",
              "      <td>0.262</td>\n",
              "      <td>0.126</td>\n",
              "      <td>0.061</td>\n",
              "      <td>5.107</td>\n",
              "      <td>2.571</td>\n",
              "      <td>2.771</td>\n",
              "      <td>0.492</td>\n",
              "      <td>10.069</td>\n",
              "      <td>7.913</td>\n",
              "      <td>6.378</td>\n",
              "      <td>2.890</td>\n",
              "      <td>9.006</td>\n",
              "      <td>9.591</td>\n",
              "      <td>4.778</td>\n",
              "      <td>5.468</td>\n",
              "      <td>6.360</td>\n",
              "      <td>9.983</td>\n",
              "      <td>4.274</td>\n",
              "      <td>2.979</td>\n",
              "      <td>0.865</td>\n",
              "      <td>5.641</td>\n",
              "      <td>0.964</td>\n",
              "      <td>0.289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100.4</td>\n",
              "      <td>118.6</td>\n",
              "      <td>142.9</td>\n",
              "      <td>31.453</td>\n",
              "      <td>0.224</td>\n",
              "      <td>0.105</td>\n",
              "      <td>0.060</td>\n",
              "      <td>4.894</td>\n",
              "      <td>2.497</td>\n",
              "      <td>2.650</td>\n",
              "      <td>0.469</td>\n",
              "      <td>10.015</td>\n",
              "      <td>7.685</td>\n",
              "      <td>6.376</td>\n",
              "      <td>2.866</td>\n",
              "      <td>9.133</td>\n",
              "      <td>9.708</td>\n",
              "      <td>4.889</td>\n",
              "      <td>5.510</td>\n",
              "      <td>6.444</td>\n",
              "      <td>10.182</td>\n",
              "      <td>4.420</td>\n",
              "      <td>2.964</td>\n",
              "      <td>0.830</td>\n",
              "      <td>5.637</td>\n",
              "      <td>0.968</td>\n",
              "      <td>0.281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100.4</td>\n",
              "      <td>118.1</td>\n",
              "      <td>142.2</td>\n",
              "      <td>32.190</td>\n",
              "      <td>0.243</td>\n",
              "      <td>0.117</td>\n",
              "      <td>0.061</td>\n",
              "      <td>4.946</td>\n",
              "      <td>2.503</td>\n",
              "      <td>2.695</td>\n",
              "      <td>0.477</td>\n",
              "      <td>10.053</td>\n",
              "      <td>7.765</td>\n",
              "      <td>6.394</td>\n",
              "      <td>2.877</td>\n",
              "      <td>9.101</td>\n",
              "      <td>9.676</td>\n",
              "      <td>4.855</td>\n",
              "      <td>5.500</td>\n",
              "      <td>6.416</td>\n",
              "      <td>10.115</td>\n",
              "      <td>4.347</td>\n",
              "      <td>3.725</td>\n",
              "      <td>0.835</td>\n",
              "      <td>4.823</td>\n",
              "      <td>0.969</td>\n",
              "      <td>0.290</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     T10    T50    T90     N+A   C5NP  ...    C9A  C10NP  C10IP   C10N   C10A\n",
              "0  100.5  119.2  146.5  31.978  0.272  ...  3.661  0.875  5.257  0.525  0.319\n",
              "1   99.6  117.9  145.5  31.568  0.297  ...  2.995  0.881  5.591  1.119  0.303\n",
              "2  100.0  118.8  145.6  31.344  0.262  ...  2.979  0.865  5.641  0.964  0.289\n",
              "3  100.4  118.6  142.9  31.453  0.224  ...  2.964  0.830  5.637  0.968  0.281\n",
              "4  100.4  118.1  142.2  32.190  0.243  ...  3.725  0.835  4.823  0.969  0.290\n",
              "\n",
              "[5 rows x 27 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SF1jJEEMf2gr",
        "colab_type": "text"
      },
      "source": [
        "# define columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nhuix1Mbfybb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_cols = df.columns.tolist()[:4]\n",
        "t_cols = df.columns.tolist()[:3]\n",
        "y_cols = df.columns.tolist()[4:]\n",
        "g_cols = ['C5N',\n",
        "          'C6N','C6A',\n",
        "          'C7N','C7A',\n",
        "          'C8N','C8A',\n",
        "          'C9N','C9A',\n",
        "          'C10N','C10A']\n",
        "h_cols =  ['C5NP','C5IP',\n",
        "           'C6NP','C6IP',\n",
        "           'C7NP','C7IP',\n",
        "           'C8NP','C8IP',\n",
        "           'C9NP','C9IP',\n",
        "           'C10NP','C10IP']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD-z6rQDwLqS",
        "colab_type": "text"
      },
      "source": [
        "# scale"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8brqYFq7wLyg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "20444d1e-bbaf-4b86-a212-5d91bb4ffe10"
      },
      "source": [
        "df['raw_NA'] = df['N+A'].values\n",
        "ss = StandardScaler()\n",
        "df[x_cols] = ss.fit_transform(df[x_cols])\n",
        "df = df[x_cols+['raw_NA']+g_cols+h_cols]\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>T10</th>\n",
              "      <th>T50</th>\n",
              "      <th>T90</th>\n",
              "      <th>N+A</th>\n",
              "      <th>raw_NA</th>\n",
              "      <th>C5N</th>\n",
              "      <th>C6N</th>\n",
              "      <th>C6A</th>\n",
              "      <th>C7N</th>\n",
              "      <th>C7A</th>\n",
              "      <th>C8N</th>\n",
              "      <th>C8A</th>\n",
              "      <th>C9N</th>\n",
              "      <th>C9A</th>\n",
              "      <th>C10N</th>\n",
              "      <th>C10A</th>\n",
              "      <th>C5NP</th>\n",
              "      <th>C5IP</th>\n",
              "      <th>C6NP</th>\n",
              "      <th>C6IP</th>\n",
              "      <th>C7NP</th>\n",
              "      <th>C7IP</th>\n",
              "      <th>C8NP</th>\n",
              "      <th>C8IP</th>\n",
              "      <th>C9NP</th>\n",
              "      <th>C9IP</th>\n",
              "      <th>C10NP</th>\n",
              "      <th>C10IP</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.435162</td>\n",
              "      <td>0.540969</td>\n",
              "      <td>0.471798</td>\n",
              "      <td>-1.028321</td>\n",
              "      <td>31.978</td>\n",
              "      <td>0.059</td>\n",
              "      <td>2.819</td>\n",
              "      <td>0.494</td>\n",
              "      <td>6.411</td>\n",
              "      <td>2.917</td>\n",
              "      <td>4.810</td>\n",
              "      <td>5.373</td>\n",
              "      <td>4.590</td>\n",
              "      <td>3.661</td>\n",
              "      <td>0.525</td>\n",
              "      <td>0.319</td>\n",
              "      <td>0.272</td>\n",
              "      <td>0.132</td>\n",
              "      <td>5.293</td>\n",
              "      <td>2.570</td>\n",
              "      <td>10.395</td>\n",
              "      <td>8.070</td>\n",
              "      <td>9.138</td>\n",
              "      <td>9.649</td>\n",
              "      <td>6.405</td>\n",
              "      <td>9.759</td>\n",
              "      <td>0.875</td>\n",
              "      <td>5.257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.353320</td>\n",
              "      <td>0.340075</td>\n",
              "      <td>0.237602</td>\n",
              "      <td>-1.077598</td>\n",
              "      <td>31.568</td>\n",
              "      <td>0.062</td>\n",
              "      <td>2.804</td>\n",
              "      <td>0.499</td>\n",
              "      <td>6.395</td>\n",
              "      <td>2.894</td>\n",
              "      <td>4.753</td>\n",
              "      <td>5.443</td>\n",
              "      <td>4.301</td>\n",
              "      <td>2.995</td>\n",
              "      <td>1.119</td>\n",
              "      <td>0.303</td>\n",
              "      <td>0.297</td>\n",
              "      <td>0.151</td>\n",
              "      <td>5.089</td>\n",
              "      <td>2.531</td>\n",
              "      <td>10.074</td>\n",
              "      <td>7.958</td>\n",
              "      <td>8.970</td>\n",
              "      <td>9.548</td>\n",
              "      <td>6.324</td>\n",
              "      <td>9.899</td>\n",
              "      <td>0.881</td>\n",
              "      <td>5.591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.389694</td>\n",
              "      <td>0.479156</td>\n",
              "      <td>0.261022</td>\n",
              "      <td>-1.104520</td>\n",
              "      <td>31.344</td>\n",
              "      <td>0.061</td>\n",
              "      <td>2.771</td>\n",
              "      <td>0.492</td>\n",
              "      <td>6.378</td>\n",
              "      <td>2.890</td>\n",
              "      <td>4.778</td>\n",
              "      <td>5.468</td>\n",
              "      <td>4.274</td>\n",
              "      <td>2.979</td>\n",
              "      <td>0.964</td>\n",
              "      <td>0.289</td>\n",
              "      <td>0.262</td>\n",
              "      <td>0.126</td>\n",
              "      <td>5.107</td>\n",
              "      <td>2.571</td>\n",
              "      <td>10.069</td>\n",
              "      <td>7.913</td>\n",
              "      <td>9.006</td>\n",
              "      <td>9.591</td>\n",
              "      <td>6.360</td>\n",
              "      <td>9.983</td>\n",
              "      <td>0.865</td>\n",
              "      <td>5.641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.426068</td>\n",
              "      <td>0.448249</td>\n",
              "      <td>-0.371307</td>\n",
              "      <td>-1.091419</td>\n",
              "      <td>31.453</td>\n",
              "      <td>0.060</td>\n",
              "      <td>2.650</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.376</td>\n",
              "      <td>2.866</td>\n",
              "      <td>4.889</td>\n",
              "      <td>5.510</td>\n",
              "      <td>4.420</td>\n",
              "      <td>2.964</td>\n",
              "      <td>0.968</td>\n",
              "      <td>0.281</td>\n",
              "      <td>0.224</td>\n",
              "      <td>0.105</td>\n",
              "      <td>4.894</td>\n",
              "      <td>2.497</td>\n",
              "      <td>10.015</td>\n",
              "      <td>7.685</td>\n",
              "      <td>9.133</td>\n",
              "      <td>9.708</td>\n",
              "      <td>6.444</td>\n",
              "      <td>10.182</td>\n",
              "      <td>0.830</td>\n",
              "      <td>5.637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.426068</td>\n",
              "      <td>0.370982</td>\n",
              "      <td>-0.535244</td>\n",
              "      <td>-1.002841</td>\n",
              "      <td>32.190</td>\n",
              "      <td>0.061</td>\n",
              "      <td>2.695</td>\n",
              "      <td>0.477</td>\n",
              "      <td>6.394</td>\n",
              "      <td>2.877</td>\n",
              "      <td>4.855</td>\n",
              "      <td>5.500</td>\n",
              "      <td>4.347</td>\n",
              "      <td>3.725</td>\n",
              "      <td>0.969</td>\n",
              "      <td>0.290</td>\n",
              "      <td>0.243</td>\n",
              "      <td>0.117</td>\n",
              "      <td>4.946</td>\n",
              "      <td>2.503</td>\n",
              "      <td>10.053</td>\n",
              "      <td>7.765</td>\n",
              "      <td>9.101</td>\n",
              "      <td>9.676</td>\n",
              "      <td>6.416</td>\n",
              "      <td>10.115</td>\n",
              "      <td>0.835</td>\n",
              "      <td>4.823</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        T10       T50       T90       N+A  ...   C9NP    C9IP  C10NP  C10IP\n",
              "0  0.435162  0.540969  0.471798 -1.028321  ...  6.405   9.759  0.875  5.257\n",
              "1  0.353320  0.340075  0.237602 -1.077598  ...  6.324   9.899  0.881  5.591\n",
              "2  0.389694  0.479156  0.261022 -1.104520  ...  6.360   9.983  0.865  5.641\n",
              "3  0.426068  0.448249 -0.371307 -1.091419  ...  6.444  10.182  0.830  5.637\n",
              "4  0.426068  0.370982 -0.535244 -1.002841  ...  6.416  10.115  0.835  4.823\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrScDuBNn6N4",
        "colab_type": "text"
      },
      "source": [
        "# split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4YE_Th6n6Uo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "bb9babfd-5997-4474-b5b0-5a4f77ff7b03"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df[x_cols+['raw_NA']].to_numpy()\n",
        "Y = df[g_cols+h_cols].to_numpy()\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_train.shape)\n",
        "print(Y_test.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3164, 5)\n",
            "(1357, 5)\n",
            "(3164, 23)\n",
            "(1357, 23)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-Mmx8nQogmI",
        "colab_type": "text"
      },
      "source": [
        "# Tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERz3U-5yogtP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = torch.tensor(X_train, dtype=torch.float)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float)\n",
        "Y_train = torch.tensor(Y_train, dtype=torch.float)\n",
        "Y_test = torch.tensor(Y_test, dtype=torch.float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVsd8C2IhIKB",
        "colab_type": "text"
      },
      "source": [
        "# data_iter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnOg6MeVgnQE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datasets = torch.utils.data.TensorDataset(X_train, Y_train)\n",
        "train_iter = torch.utils.data.DataLoader(datasets, batch_size=128, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7pJsRwshV6a",
        "colab_type": "text"
      },
      "source": [
        "# Construct the neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5_3YZ0KgnSw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dual_net(nn.Module):\n",
        "  def __init__(self,input_shape,g_shape,h_shape):\n",
        "    super(Dual_net,self).__init__()\n",
        "    # common fc\n",
        "    hid = 128\n",
        "    self.fc1 = nn.Linear(input_shape,hid)\n",
        "    self.fc2 = nn.Linear(hid,hid)\n",
        "    self.fc3 = nn.Linear(hid,hid)\n",
        "    # g\n",
        "    self.g0 = nn.Linear(hid,hid)\n",
        "    self.g1 = nn.Linear(hid,hid)\n",
        "    self.g2 = nn.Linear(hid,g_shape)\n",
        "    # h\n",
        "    self.h0 = nn.Linear(hid,hid)\n",
        "    self.h1 = nn.Linear(hid,hid)\n",
        "    self.h2 = nn.Linear(hid,h_shape)\n",
        "    # activation\n",
        "    self.relu = nn.ReLU()\n",
        "  \n",
        "  def forward(self,x):\n",
        "    # get raw N+A and raw other\n",
        "    g_total = x[:,4].unsqueeze(1)\n",
        "    h_total = 100 - x[:,4].unsqueeze(1)\n",
        "    # only keep scaled_t_cols and scaled_N+A \n",
        "    x = x[:,:4]\n",
        "    # common forward\n",
        "    x = self.relu(self.fc1(x))\n",
        "    x = self.relu(self.fc2(x))\n",
        "    x = self.relu(self.fc3(x))\n",
        "    # forward g and normalize\n",
        "    #g = self.relu(self.g0(x))\n",
        "    g = self.g1(x)\n",
        "    g = F.softmax(self.g2(x),dim=1)*g_total\n",
        "    # forward h and normalize\n",
        "    #h = self.relu(self.h0(x))\n",
        "    h = self.h1(x)\n",
        "    h = F.softmax(self.h2(x),dim=1)*h_total\n",
        "    # concat (g,h)\n",
        "    y = torch.cat((g,h),1)\n",
        "    return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hXS2vIDkf_q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_weights(m):\n",
        "  if type(m) == nn.Linear:\n",
        "    torch.nn.init.xavier_uniform(m.weight)\n",
        "    m.bias.data.fill_(0)\n",
        "    print('initialize {}'.format(m))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XKjjfJ7gnVN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "beeefed4-c47d-458d-cdc6-2a6b284ac7fa"
      },
      "source": [
        "net = Dual_net(4,11,12)\n",
        "net.apply(init_weights)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initialize Linear(in_features=4, out_features=128, bias=True)\n",
            "initialize Linear(in_features=128, out_features=128, bias=True)\n",
            "initialize Linear(in_features=128, out_features=128, bias=True)\n",
            "initialize Linear(in_features=128, out_features=128, bias=True)\n",
            "initialize Linear(in_features=128, out_features=128, bias=True)\n",
            "initialize Linear(in_features=128, out_features=11, bias=True)\n",
            "initialize Linear(in_features=128, out_features=128, bias=True)\n",
            "initialize Linear(in_features=128, out_features=128, bias=True)\n",
            "initialize Linear(in_features=128, out_features=12, bias=True)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dual_net(\n",
              "  (fc1): Linear(in_features=4, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (g0): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (g1): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (g2): Linear(in_features=128, out_features=11, bias=True)\n",
              "  (h0): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (h1): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (h2): Linear(in_features=128, out_features=12, bias=True)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_cRckt3mtsF",
        "colab_type": "text"
      },
      "source": [
        "# loss_function and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-pwb-GJ0Mrg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RMSELoss(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RMSELoss,self).__init__()\n",
        "\n",
        "    def forward(self,x,y):\n",
        "        criterion = nn.MSELoss()\n",
        "        loss = torch.sqrt(criterion(x, y))\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Od_3nhT3gng1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_function = RMSELoss()\n",
        "optimizer = optim.Adam(net.parameters(),lr=0.0001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FH8xJsVnEqr",
        "colab_type": "text"
      },
      "source": [
        "# Training and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLD-m6xfgnih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net,train_iter,loss_function,optimizer,num_epochs=100):\n",
        "  history = []\n",
        "  for epoch in range(num_epochs):\n",
        "    for x,y in train_iter:\n",
        "      loss = loss_function(net(x),y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "    # end for\n",
        "    print(\"epochs {} loss {:.4f}\".format(epoch,loss.item()))\n",
        "    if loss.item() < 0.365:\n",
        "      break\n",
        "    history.append(loss.item())\n",
        "  # end for\n",
        "  plt.plot(np.array(history))\n",
        "  plt.title('train loss')\n",
        "  return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzQFbEE4gtFw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d63dbf63-5373-484b-d480-67fcd95d5664"
      },
      "source": [
        "train(net,train_iter,loss_function,optimizer,num_epochs=1000)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epochs 0 loss 3.1587\n",
            "epochs 1 loss 2.8242\n",
            "epochs 2 loss 2.4605\n",
            "epochs 3 loss 2.0349\n",
            "epochs 4 loss 1.6717\n",
            "epochs 5 loss 1.5086\n",
            "epochs 6 loss 1.2516\n",
            "epochs 7 loss 1.0420\n",
            "epochs 8 loss 0.9355\n",
            "epochs 9 loss 1.0057\n",
            "epochs 10 loss 1.1523\n",
            "epochs 11 loss 0.9854\n",
            "epochs 12 loss 0.8071\n",
            "epochs 13 loss 0.8843\n",
            "epochs 14 loss 0.9616\n",
            "epochs 15 loss 0.8963\n",
            "epochs 16 loss 0.6922\n",
            "epochs 17 loss 0.9918\n",
            "epochs 18 loss 0.6196\n",
            "epochs 19 loss 0.8888\n",
            "epochs 20 loss 0.9319\n",
            "epochs 21 loss 0.7278\n",
            "epochs 22 loss 0.5839\n",
            "epochs 23 loss 0.9681\n",
            "epochs 24 loss 0.5814\n",
            "epochs 25 loss 1.1043\n",
            "epochs 26 loss 1.7068\n",
            "epochs 27 loss 0.6510\n",
            "epochs 28 loss 1.0642\n",
            "epochs 29 loss 0.5097\n",
            "epochs 30 loss 0.5484\n",
            "epochs 31 loss 0.6424\n",
            "epochs 32 loss 0.9262\n",
            "epochs 33 loss 0.5964\n",
            "epochs 34 loss 0.7206\n",
            "epochs 35 loss 1.3620\n",
            "epochs 36 loss 0.6538\n",
            "epochs 37 loss 0.7520\n",
            "epochs 38 loss 0.6034\n",
            "epochs 39 loss 1.2489\n",
            "epochs 40 loss 0.5282\n",
            "epochs 41 loss 0.7794\n",
            "epochs 42 loss 0.6823\n",
            "epochs 43 loss 0.8974\n",
            "epochs 44 loss 0.8682\n",
            "epochs 45 loss 0.5027\n",
            "epochs 46 loss 0.6935\n",
            "epochs 47 loss 0.6432\n",
            "epochs 48 loss 0.6554\n",
            "epochs 49 loss 0.5229\n",
            "epochs 50 loss 0.6775\n",
            "epochs 51 loss 0.5899\n",
            "epochs 52 loss 0.7608\n",
            "epochs 53 loss 0.7870\n",
            "epochs 54 loss 0.5701\n",
            "epochs 55 loss 0.5867\n",
            "epochs 56 loss 0.6013\n",
            "epochs 57 loss 0.6111\n",
            "epochs 58 loss 0.5078\n",
            "epochs 59 loss 0.8786\n",
            "epochs 60 loss 0.6581\n",
            "epochs 61 loss 0.8356\n",
            "epochs 62 loss 0.5502\n",
            "epochs 63 loss 0.8267\n",
            "epochs 64 loss 0.5667\n",
            "epochs 65 loss 0.6446\n",
            "epochs 66 loss 0.6362\n",
            "epochs 67 loss 0.5134\n",
            "epochs 68 loss 0.5214\n",
            "epochs 69 loss 0.6776\n",
            "epochs 70 loss 0.4876\n",
            "epochs 71 loss 0.5105\n",
            "epochs 72 loss 1.0885\n",
            "epochs 73 loss 0.6525\n",
            "epochs 74 loss 0.4868\n",
            "epochs 75 loss 0.5238\n",
            "epochs 76 loss 0.5877\n",
            "epochs 77 loss 0.5466\n",
            "epochs 78 loss 0.4578\n",
            "epochs 79 loss 0.4598\n",
            "epochs 80 loss 0.5692\n",
            "epochs 81 loss 0.5612\n",
            "epochs 82 loss 1.0072\n",
            "epochs 83 loss 0.6578\n",
            "epochs 84 loss 0.5489\n",
            "epochs 85 loss 0.6695\n",
            "epochs 86 loss 0.9919\n",
            "epochs 87 loss 0.4735\n",
            "epochs 88 loss 0.5544\n",
            "epochs 89 loss 0.5357\n",
            "epochs 90 loss 0.4659\n",
            "epochs 91 loss 1.2576\n",
            "epochs 92 loss 0.9333\n",
            "epochs 93 loss 0.5131\n",
            "epochs 94 loss 0.5151\n",
            "epochs 95 loss 0.4630\n",
            "epochs 96 loss 0.6719\n",
            "epochs 97 loss 0.6939\n",
            "epochs 98 loss 0.5281\n",
            "epochs 99 loss 0.6570\n",
            "epochs 100 loss 0.8290\n",
            "epochs 101 loss 0.5246\n",
            "epochs 102 loss 0.5919\n",
            "epochs 103 loss 0.6308\n",
            "epochs 104 loss 0.5632\n",
            "epochs 105 loss 0.5039\n",
            "epochs 106 loss 0.6535\n",
            "epochs 107 loss 0.8716\n",
            "epochs 108 loss 0.5867\n",
            "epochs 109 loss 0.9932\n",
            "epochs 110 loss 0.4383\n",
            "epochs 111 loss 0.5104\n",
            "epochs 112 loss 0.6479\n",
            "epochs 113 loss 0.4980\n",
            "epochs 114 loss 0.4632\n",
            "epochs 115 loss 0.6430\n",
            "epochs 116 loss 0.5203\n",
            "epochs 117 loss 0.5412\n",
            "epochs 118 loss 0.5860\n",
            "epochs 119 loss 0.4542\n",
            "epochs 120 loss 0.7530\n",
            "epochs 121 loss 0.5374\n",
            "epochs 122 loss 0.5888\n",
            "epochs 123 loss 0.4892\n",
            "epochs 124 loss 0.5428\n",
            "epochs 125 loss 0.5909\n",
            "epochs 126 loss 0.5951\n",
            "epochs 127 loss 0.8288\n",
            "epochs 128 loss 0.6242\n",
            "epochs 129 loss 0.4995\n",
            "epochs 130 loss 0.6776\n",
            "epochs 131 loss 0.4117\n",
            "epochs 132 loss 0.4552\n",
            "epochs 133 loss 0.8065\n",
            "epochs 134 loss 0.5387\n",
            "epochs 135 loss 0.4448\n",
            "epochs 136 loss 0.6150\n",
            "epochs 137 loss 0.5266\n",
            "epochs 138 loss 0.6762\n",
            "epochs 139 loss 0.4336\n",
            "epochs 140 loss 0.4583\n",
            "epochs 141 loss 0.9254\n",
            "epochs 142 loss 0.4209\n",
            "epochs 143 loss 0.8810\n",
            "epochs 144 loss 0.5075\n",
            "epochs 145 loss 0.4983\n",
            "epochs 146 loss 0.5816\n",
            "epochs 147 loss 0.5774\n",
            "epochs 148 loss 0.9315\n",
            "epochs 149 loss 0.9938\n",
            "epochs 150 loss 0.4390\n",
            "epochs 151 loss 0.8607\n",
            "epochs 152 loss 0.5426\n",
            "epochs 153 loss 1.0068\n",
            "epochs 154 loss 0.6751\n",
            "epochs 155 loss 0.4757\n",
            "epochs 156 loss 0.6015\n",
            "epochs 157 loss 0.6032\n",
            "epochs 158 loss 0.6663\n",
            "epochs 159 loss 0.7105\n",
            "epochs 160 loss 0.5920\n",
            "epochs 161 loss 0.4726\n",
            "epochs 162 loss 0.5793\n",
            "epochs 163 loss 0.8351\n",
            "epochs 164 loss 0.6200\n",
            "epochs 165 loss 0.6273\n",
            "epochs 166 loss 0.4612\n",
            "epochs 167 loss 0.7358\n",
            "epochs 168 loss 0.6138\n",
            "epochs 169 loss 0.4630\n",
            "epochs 170 loss 0.5939\n",
            "epochs 171 loss 0.4225\n",
            "epochs 172 loss 0.5431\n",
            "epochs 173 loss 0.5559\n",
            "epochs 174 loss 0.5260\n",
            "epochs 175 loss 0.9821\n",
            "epochs 176 loss 0.7416\n",
            "epochs 177 loss 0.5766\n",
            "epochs 178 loss 0.5478\n",
            "epochs 179 loss 0.9117\n",
            "epochs 180 loss 0.4868\n",
            "epochs 181 loss 0.4510\n",
            "epochs 182 loss 0.6251\n",
            "epochs 183 loss 0.5291\n",
            "epochs 184 loss 0.4721\n",
            "epochs 185 loss 0.4562\n",
            "epochs 186 loss 0.5013\n",
            "epochs 187 loss 0.7067\n",
            "epochs 188 loss 0.6756\n",
            "epochs 189 loss 0.5743\n",
            "epochs 190 loss 0.4872\n",
            "epochs 191 loss 0.9373\n",
            "epochs 192 loss 0.5300\n",
            "epochs 193 loss 1.0400\n",
            "epochs 194 loss 0.5572\n",
            "epochs 195 loss 0.6387\n",
            "epochs 196 loss 0.7196\n",
            "epochs 197 loss 0.4769\n",
            "epochs 198 loss 0.5349\n",
            "epochs 199 loss 0.9306\n",
            "epochs 200 loss 0.6453\n",
            "epochs 201 loss 0.5544\n",
            "epochs 202 loss 0.4162\n",
            "epochs 203 loss 0.5673\n",
            "epochs 204 loss 0.6125\n",
            "epochs 205 loss 0.5686\n",
            "epochs 206 loss 0.6842\n",
            "epochs 207 loss 0.4952\n",
            "epochs 208 loss 0.5336\n",
            "epochs 209 loss 0.5542\n",
            "epochs 210 loss 0.3801\n",
            "epochs 211 loss 0.4947\n",
            "epochs 212 loss 0.4534\n",
            "epochs 213 loss 0.5180\n",
            "epochs 214 loss 0.4578\n",
            "epochs 215 loss 0.6742\n",
            "epochs 216 loss 0.5153\n",
            "epochs 217 loss 0.7162\n",
            "epochs 218 loss 0.4354\n",
            "epochs 219 loss 0.6064\n",
            "epochs 220 loss 0.6813\n",
            "epochs 221 loss 0.7052\n",
            "epochs 222 loss 0.4441\n",
            "epochs 223 loss 0.8266\n",
            "epochs 224 loss 0.5622\n",
            "epochs 225 loss 0.3825\n",
            "epochs 226 loss 0.4808\n",
            "epochs 227 loss 0.6312\n",
            "epochs 228 loss 0.4865\n",
            "epochs 229 loss 0.5753\n",
            "epochs 230 loss 0.4228\n",
            "epochs 231 loss 0.7213\n",
            "epochs 232 loss 0.6515\n",
            "epochs 233 loss 0.6627\n",
            "epochs 234 loss 0.6634\n",
            "epochs 235 loss 0.7065\n",
            "epochs 236 loss 0.4264\n",
            "epochs 237 loss 0.5249\n",
            "epochs 238 loss 0.9682\n",
            "epochs 239 loss 0.4433\n",
            "epochs 240 loss 0.6041\n",
            "epochs 241 loss 0.5282\n",
            "epochs 242 loss 0.4879\n",
            "epochs 243 loss 0.4779\n",
            "epochs 244 loss 0.3682\n",
            "epochs 245 loss 0.4978\n",
            "epochs 246 loss 0.4175\n",
            "epochs 247 loss 0.5858\n",
            "epochs 248 loss 0.6707\n",
            "epochs 249 loss 0.8830\n",
            "epochs 250 loss 0.3906\n",
            "epochs 251 loss 0.6404\n",
            "epochs 252 loss 0.5029\n",
            "epochs 253 loss 0.5399\n",
            "epochs 254 loss 0.4837\n",
            "epochs 255 loss 0.4693\n",
            "epochs 256 loss 0.5259\n",
            "epochs 257 loss 0.9083\n",
            "epochs 258 loss 0.5068\n",
            "epochs 259 loss 0.4742\n",
            "epochs 260 loss 0.8892\n",
            "epochs 261 loss 0.5762\n",
            "epochs 262 loss 0.5641\n",
            "epochs 263 loss 0.7414\n",
            "epochs 264 loss 0.5102\n",
            "epochs 265 loss 0.5624\n",
            "epochs 266 loss 0.7125\n",
            "epochs 267 loss 0.4074\n",
            "epochs 268 loss 0.6160\n",
            "epochs 269 loss 0.6040\n",
            "epochs 270 loss 0.6807\n",
            "epochs 271 loss 0.5782\n",
            "epochs 272 loss 0.4507\n",
            "epochs 273 loss 0.6140\n",
            "epochs 274 loss 0.6708\n",
            "epochs 275 loss 0.9147\n",
            "epochs 276 loss 0.4859\n",
            "epochs 277 loss 0.5621\n",
            "epochs 278 loss 0.6232\n",
            "epochs 279 loss 0.7051\n",
            "epochs 280 loss 0.4473\n",
            "epochs 281 loss 0.4511\n",
            "epochs 282 loss 0.7191\n",
            "epochs 283 loss 0.4594\n",
            "epochs 284 loss 0.5795\n",
            "epochs 285 loss 0.6660\n",
            "epochs 286 loss 0.5005\n",
            "epochs 287 loss 0.6338\n",
            "epochs 288 loss 0.9530\n",
            "epochs 289 loss 0.5482\n",
            "epochs 290 loss 1.1151\n",
            "epochs 291 loss 0.8918\n",
            "epochs 292 loss 0.4462\n",
            "epochs 293 loss 0.5581\n",
            "epochs 294 loss 0.3770\n",
            "epochs 295 loss 0.5321\n",
            "epochs 296 loss 0.5763\n",
            "epochs 297 loss 0.4725\n",
            "epochs 298 loss 0.9022\n",
            "epochs 299 loss 0.3964\n",
            "epochs 300 loss 0.4790\n",
            "epochs 301 loss 0.8861\n",
            "epochs 302 loss 0.5259\n",
            "epochs 303 loss 0.4955\n",
            "epochs 304 loss 0.5134\n",
            "epochs 305 loss 0.6215\n",
            "epochs 306 loss 0.4296\n",
            "epochs 307 loss 0.8605\n",
            "epochs 308 loss 0.4199\n",
            "epochs 309 loss 0.4173\n",
            "epochs 310 loss 0.4403\n",
            "epochs 311 loss 0.5565\n",
            "epochs 312 loss 0.4372\n",
            "epochs 313 loss 0.4991\n",
            "epochs 314 loss 0.5435\n",
            "epochs 315 loss 0.4739\n",
            "epochs 316 loss 0.9853\n",
            "epochs 317 loss 0.4486\n",
            "epochs 318 loss 0.5282\n",
            "epochs 319 loss 0.6678\n",
            "epochs 320 loss 0.6575\n",
            "epochs 321 loss 0.4754\n",
            "epochs 322 loss 1.0067\n",
            "epochs 323 loss 0.4678\n",
            "epochs 324 loss 0.4111\n",
            "epochs 325 loss 0.4482\n",
            "epochs 326 loss 0.5536\n",
            "epochs 327 loss 0.4392\n",
            "epochs 328 loss 0.6134\n",
            "epochs 329 loss 0.6011\n",
            "epochs 330 loss 0.4613\n",
            "epochs 331 loss 0.5934\n",
            "epochs 332 loss 0.7792\n",
            "epochs 333 loss 0.4513\n",
            "epochs 334 loss 0.6714\n",
            "epochs 335 loss 0.5180\n",
            "epochs 336 loss 0.5735\n",
            "epochs 337 loss 0.5216\n",
            "epochs 338 loss 1.0171\n",
            "epochs 339 loss 1.2222\n",
            "epochs 340 loss 1.0451\n",
            "epochs 341 loss 0.4407\n",
            "epochs 342 loss 0.4563\n",
            "epochs 343 loss 0.4732\n",
            "epochs 344 loss 0.4798\n",
            "epochs 345 loss 0.6804\n",
            "epochs 346 loss 0.6793\n",
            "epochs 347 loss 0.3779\n",
            "epochs 348 loss 0.4818\n",
            "epochs 349 loss 0.4627\n",
            "epochs 350 loss 0.4326\n",
            "epochs 351 loss 0.4914\n",
            "epochs 352 loss 0.4738\n",
            "epochs 353 loss 0.5048\n",
            "epochs 354 loss 0.4984\n",
            "epochs 355 loss 0.4991\n",
            "epochs 356 loss 0.5112\n",
            "epochs 357 loss 0.4418\n",
            "epochs 358 loss 0.6331\n",
            "epochs 359 loss 0.6006\n",
            "epochs 360 loss 0.4752\n",
            "epochs 361 loss 0.4788\n",
            "epochs 362 loss 0.4643\n",
            "epochs 363 loss 0.5045\n",
            "epochs 364 loss 0.4030\n",
            "epochs 365 loss 0.5049\n",
            "epochs 366 loss 0.4895\n",
            "epochs 367 loss 0.4848\n",
            "epochs 368 loss 0.7100\n",
            "epochs 369 loss 0.6651\n",
            "epochs 370 loss 0.5944\n",
            "epochs 371 loss 0.4812\n",
            "epochs 372 loss 0.5342\n",
            "epochs 373 loss 0.6526\n",
            "epochs 374 loss 0.4301\n",
            "epochs 375 loss 0.4425\n",
            "epochs 376 loss 0.6745\n",
            "epochs 377 loss 0.4364\n",
            "epochs 378 loss 0.4591\n",
            "epochs 379 loss 0.4180\n",
            "epochs 380 loss 0.3914\n",
            "epochs 381 loss 0.4023\n",
            "epochs 382 loss 0.6059\n",
            "epochs 383 loss 0.5424\n",
            "epochs 384 loss 0.5544\n",
            "epochs 385 loss 1.2524\n",
            "epochs 386 loss 0.6429\n",
            "epochs 387 loss 0.3725\n",
            "epochs 388 loss 0.4658\n",
            "epochs 389 loss 0.5543\n",
            "epochs 390 loss 0.6949\n",
            "epochs 391 loss 0.6784\n",
            "epochs 392 loss 0.5208\n",
            "epochs 393 loss 0.5004\n",
            "epochs 394 loss 0.5403\n",
            "epochs 395 loss 0.6843\n",
            "epochs 396 loss 0.5395\n",
            "epochs 397 loss 0.6244\n",
            "epochs 398 loss 0.5001\n",
            "epochs 399 loss 0.5368\n",
            "epochs 400 loss 0.4617\n",
            "epochs 401 loss 0.5869\n",
            "epochs 402 loss 0.7285\n",
            "epochs 403 loss 0.4795\n",
            "epochs 404 loss 0.7412\n",
            "epochs 405 loss 0.3961\n",
            "epochs 406 loss 0.4686\n",
            "epochs 407 loss 0.4383\n",
            "epochs 408 loss 0.6351\n",
            "epochs 409 loss 0.5875\n",
            "epochs 410 loss 0.3911\n",
            "epochs 411 loss 0.4132\n",
            "epochs 412 loss 0.4812\n",
            "epochs 413 loss 0.5162\n",
            "epochs 414 loss 0.5501\n",
            "epochs 415 loss 0.5639\n",
            "epochs 416 loss 0.4156\n",
            "epochs 417 loss 0.9326\n",
            "epochs 418 loss 0.5831\n",
            "epochs 419 loss 0.4436\n",
            "epochs 420 loss 0.4440\n",
            "epochs 421 loss 0.3692\n",
            "epochs 422 loss 0.5762\n",
            "epochs 423 loss 0.3574\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dual_net(\n",
              "  (fc1): Linear(in_features=4, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (g0): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (g1): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (g2): Linear(in_features=128, out_features=11, bias=True)\n",
              "  (h0): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (h1): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (h2): Linear(in_features=128, out_features=12, bias=True)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4vCKTKGpacL",
        "colab_type": "text"
      },
      "source": [
        "# predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cvd94KrEgtHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = net(X_test).detach().numpy()\n",
        "y_real = Y_test.detach().numpy()\n",
        "y_pred = pd.DataFrame(y_pred,columns=g_cols+h_cols)[y_cols]\n",
        "y_real = pd.DataFrame(y_real,columns=g_cols+h_cols)[y_cols]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvuDtKyNgtLR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "abf693a0-288a-4c4a-cab6-5315bbcf4105"
      },
      "source": [
        "y_pred.head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>C5NP</th>\n",
              "      <th>C5IP</th>\n",
              "      <th>C5N</th>\n",
              "      <th>C6NP</th>\n",
              "      <th>C6IP</th>\n",
              "      <th>C6N</th>\n",
              "      <th>C6A</th>\n",
              "      <th>C7NP</th>\n",
              "      <th>C7IP</th>\n",
              "      <th>C7N</th>\n",
              "      <th>C7A</th>\n",
              "      <th>C8NP</th>\n",
              "      <th>C8IP</th>\n",
              "      <th>C8N</th>\n",
              "      <th>C8A</th>\n",
              "      <th>C9NP</th>\n",
              "      <th>C9IP</th>\n",
              "      <th>C9N</th>\n",
              "      <th>C9A</th>\n",
              "      <th>C10NP</th>\n",
              "      <th>C10IP</th>\n",
              "      <th>C10N</th>\n",
              "      <th>C10A</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.461484</td>\n",
              "      <td>0.322669</td>\n",
              "      <td>0.051969</td>\n",
              "      <td>4.464067</td>\n",
              "      <td>1.957693</td>\n",
              "      <td>2.735931</td>\n",
              "      <td>0.654521</td>\n",
              "      <td>10.492964</td>\n",
              "      <td>7.919146</td>\n",
              "      <td>6.456284</td>\n",
              "      <td>2.973159</td>\n",
              "      <td>9.533070</td>\n",
              "      <td>10.126275</td>\n",
              "      <td>5.097260</td>\n",
              "      <td>5.328704</td>\n",
              "      <td>6.342155</td>\n",
              "      <td>10.071034</td>\n",
              "      <td>4.556020</td>\n",
              "      <td>3.322922</td>\n",
              "      <td>1.006869</td>\n",
              "      <td>5.096565</td>\n",
              "      <td>0.546954</td>\n",
              "      <td>0.482276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9.242173</td>\n",
              "      <td>6.764837</td>\n",
              "      <td>1.094550</td>\n",
              "      <td>6.442904</td>\n",
              "      <td>7.656085</td>\n",
              "      <td>6.654846</td>\n",
              "      <td>2.356804</td>\n",
              "      <td>5.604897</td>\n",
              "      <td>5.697542</td>\n",
              "      <td>10.263255</td>\n",
              "      <td>3.295980</td>\n",
              "      <td>4.546033</td>\n",
              "      <td>4.403881</td>\n",
              "      <td>5.462707</td>\n",
              "      <td>3.649579</td>\n",
              "      <td>2.603800</td>\n",
              "      <td>4.926518</td>\n",
              "      <td>2.938269</td>\n",
              "      <td>1.570851</td>\n",
              "      <td>1.012041</td>\n",
              "      <td>2.649282</td>\n",
              "      <td>0.491503</td>\n",
              "      <td>0.671656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.245844</td>\n",
              "      <td>0.161725</td>\n",
              "      <td>0.032358</td>\n",
              "      <td>3.346081</td>\n",
              "      <td>1.139460</td>\n",
              "      <td>2.314285</td>\n",
              "      <td>0.537633</td>\n",
              "      <td>10.917281</td>\n",
              "      <td>8.177757</td>\n",
              "      <td>6.818314</td>\n",
              "      <td>3.022992</td>\n",
              "      <td>9.826222</td>\n",
              "      <td>10.422038</td>\n",
              "      <td>5.426436</td>\n",
              "      <td>5.678351</td>\n",
              "      <td>6.524893</td>\n",
              "      <td>10.654734</td>\n",
              "      <td>4.683532</td>\n",
              "      <td>3.377324</td>\n",
              "      <td>0.760766</td>\n",
              "      <td>5.016205</td>\n",
              "      <td>0.500544</td>\n",
              "      <td>0.415228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.291603</td>\n",
              "      <td>0.199313</td>\n",
              "      <td>0.032299</td>\n",
              "      <td>3.690057</td>\n",
              "      <td>1.315148</td>\n",
              "      <td>2.327674</td>\n",
              "      <td>0.524474</td>\n",
              "      <td>10.854421</td>\n",
              "      <td>8.014260</td>\n",
              "      <td>6.483541</td>\n",
              "      <td>2.973301</td>\n",
              "      <td>9.856676</td>\n",
              "      <td>10.427977</td>\n",
              "      <td>5.161060</td>\n",
              "      <td>5.450098</td>\n",
              "      <td>6.596172</td>\n",
              "      <td>10.579491</td>\n",
              "      <td>4.562065</td>\n",
              "      <td>3.481971</td>\n",
              "      <td>0.906169</td>\n",
              "      <td>5.244709</td>\n",
              "      <td>0.528495</td>\n",
              "      <td>0.499022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.328790</td>\n",
              "      <td>1.395031</td>\n",
              "      <td>0.305742</td>\n",
              "      <td>2.983939</td>\n",
              "      <td>2.803380</td>\n",
              "      <td>5.323826</td>\n",
              "      <td>0.520345</td>\n",
              "      <td>5.846519</td>\n",
              "      <td>5.897274</td>\n",
              "      <td>18.463678</td>\n",
              "      <td>3.512871</td>\n",
              "      <td>5.672984</td>\n",
              "      <td>6.552239</td>\n",
              "      <td>9.775188</td>\n",
              "      <td>4.652979</td>\n",
              "      <td>2.953038</td>\n",
              "      <td>7.190737</td>\n",
              "      <td>5.344854</td>\n",
              "      <td>2.199462</td>\n",
              "      <td>0.799945</td>\n",
              "      <td>4.105123</td>\n",
              "      <td>0.715150</td>\n",
              "      <td>0.656900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       C5NP      C5IP       C5N  ...     C10IP      C10N      C10A\n",
              "0  0.461484  0.322669  0.051969  ...  5.096565  0.546954  0.482276\n",
              "1  9.242173  6.764837  1.094550  ...  2.649282  0.491503  0.671656\n",
              "2  0.245844  0.161725  0.032358  ...  5.016205  0.500544  0.415228\n",
              "3  0.291603  0.199313  0.032299  ...  5.244709  0.528495  0.499022\n",
              "4  2.328790  1.395031  0.305742  ...  4.105123  0.715150  0.656900\n",
              "\n",
              "[5 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZ5G24bvgtNf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f2b592b7-e6e3-47f6-d34d-acba29e15727"
      },
      "source": [
        "y_real.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>C5NP</th>\n",
              "      <th>C5IP</th>\n",
              "      <th>C5N</th>\n",
              "      <th>C6NP</th>\n",
              "      <th>C6IP</th>\n",
              "      <th>C6N</th>\n",
              "      <th>C6A</th>\n",
              "      <th>C7NP</th>\n",
              "      <th>C7IP</th>\n",
              "      <th>C7N</th>\n",
              "      <th>C7A</th>\n",
              "      <th>C8NP</th>\n",
              "      <th>C8IP</th>\n",
              "      <th>C8N</th>\n",
              "      <th>C8A</th>\n",
              "      <th>C9NP</th>\n",
              "      <th>C9IP</th>\n",
              "      <th>C9N</th>\n",
              "      <th>C9A</th>\n",
              "      <th>C10NP</th>\n",
              "      <th>C10IP</th>\n",
              "      <th>C10N</th>\n",
              "      <th>C10A</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.428</td>\n",
              "      <td>0.242</td>\n",
              "      <td>0.052</td>\n",
              "      <td>4.339</td>\n",
              "      <td>1.800</td>\n",
              "      <td>2.475</td>\n",
              "      <td>0.488</td>\n",
              "      <td>10.614</td>\n",
              "      <td>7.778</td>\n",
              "      <td>6.648000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>9.557</td>\n",
              "      <td>10.257</td>\n",
              "      <td>5.017</td>\n",
              "      <td>5.354</td>\n",
              "      <td>6.320</td>\n",
              "      <td>9.887</td>\n",
              "      <td>4.828</td>\n",
              "      <td>3.368</td>\n",
              "      <td>0.919</td>\n",
              "      <td>5.034</td>\n",
              "      <td>0.541</td>\n",
              "      <td>0.435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7.850</td>\n",
              "      <td>5.467</td>\n",
              "      <td>0.780</td>\n",
              "      <td>7.012</td>\n",
              "      <td>7.951</td>\n",
              "      <td>7.060</td>\n",
              "      <td>2.426</td>\n",
              "      <td>5.701</td>\n",
              "      <td>6.196</td>\n",
              "      <td>10.217000</td>\n",
              "      <td>3.160</td>\n",
              "      <td>4.513</td>\n",
              "      <td>4.859</td>\n",
              "      <td>5.342</td>\n",
              "      <td>3.367</td>\n",
              "      <td>2.464</td>\n",
              "      <td>4.781</td>\n",
              "      <td>3.126</td>\n",
              "      <td>1.654</td>\n",
              "      <td>1.081</td>\n",
              "      <td>2.849</td>\n",
              "      <td>0.559</td>\n",
              "      <td>0.759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.233</td>\n",
              "      <td>0.175</td>\n",
              "      <td>0.025</td>\n",
              "      <td>3.026</td>\n",
              "      <td>0.786</td>\n",
              "      <td>2.314</td>\n",
              "      <td>0.389</td>\n",
              "      <td>10.812</td>\n",
              "      <td>7.907</td>\n",
              "      <td>7.313000</td>\n",
              "      <td>3.213</td>\n",
              "      <td>9.748</td>\n",
              "      <td>10.480</td>\n",
              "      <td>5.221</td>\n",
              "      <td>5.787</td>\n",
              "      <td>6.614</td>\n",
              "      <td>10.756</td>\n",
              "      <td>4.250</td>\n",
              "      <td>3.384</td>\n",
              "      <td>0.735</td>\n",
              "      <td>4.688</td>\n",
              "      <td>0.585</td>\n",
              "      <td>0.326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.046</td>\n",
              "      <td>0.024</td>\n",
              "      <td>0.012</td>\n",
              "      <td>3.892</td>\n",
              "      <td>1.266</td>\n",
              "      <td>2.199</td>\n",
              "      <td>0.401</td>\n",
              "      <td>10.848</td>\n",
              "      <td>7.669</td>\n",
              "      <td>6.472000</td>\n",
              "      <td>3.082</td>\n",
              "      <td>10.318</td>\n",
              "      <td>11.059</td>\n",
              "      <td>5.208</td>\n",
              "      <td>5.487</td>\n",
              "      <td>6.508</td>\n",
              "      <td>10.085</td>\n",
              "      <td>4.992</td>\n",
              "      <td>3.303</td>\n",
              "      <td>0.815</td>\n",
              "      <td>4.901</td>\n",
              "      <td>0.499</td>\n",
              "      <td>0.369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.306</td>\n",
              "      <td>1.492</td>\n",
              "      <td>0.277</td>\n",
              "      <td>2.732</td>\n",
              "      <td>2.560</td>\n",
              "      <td>4.559</td>\n",
              "      <td>0.609</td>\n",
              "      <td>5.690</td>\n",
              "      <td>5.916</td>\n",
              "      <td>18.572001</td>\n",
              "      <td>3.592</td>\n",
              "      <td>5.128</td>\n",
              "      <td>6.858</td>\n",
              "      <td>9.576</td>\n",
              "      <td>4.610</td>\n",
              "      <td>2.546</td>\n",
              "      <td>7.436</td>\n",
              "      <td>5.706</td>\n",
              "      <td>2.365</td>\n",
              "      <td>0.747</td>\n",
              "      <td>4.039</td>\n",
              "      <td>0.823</td>\n",
              "      <td>0.782</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    C5NP   C5IP    C5N   C6NP   C6IP  ...    C9A  C10NP  C10IP   C10N   C10A\n",
              "0  0.428  0.242  0.052  4.339  1.800  ...  3.368  0.919  5.034  0.541  0.435\n",
              "1  7.850  5.467  0.780  7.012  7.951  ...  1.654  1.081  2.849  0.559  0.759\n",
              "2  0.233  0.175  0.025  3.026  0.786  ...  3.384  0.735  4.688  0.585  0.326\n",
              "3  0.046  0.024  0.012  3.892  1.266  ...  3.303  0.815  4.901  0.499  0.369\n",
              "4  2.306  1.492  0.277  2.732  2.560  ...  2.365  0.747  4.039  0.823  0.782\n",
              "\n",
              "[5 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG5bJe9BpJYC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        },
        "outputId": "c0f48085-0379-41fe-9371-f59fd6c0965a"
      },
      "source": [
        "res = pd.DataFrame(index=y_cols,columns=['R2','RMSE'])\n",
        "for i in y_cols:\n",
        "  res.loc[i,'R2'] = r2_score(y_real[i],y_pred[i])\n",
        "  res.loc[i,'RMSE'] = sqrt(mean_squared_error(y_real[i],y_pred[i]))\n",
        "res.loc['AVG'] = res.mean(axis=0)\n",
        "res"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>R2</th>\n",
              "      <th>RMSE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>C5NP</th>\n",
              "      <td>0.900445</td>\n",
              "      <td>0.696566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C5IP</th>\n",
              "      <td>0.84799</td>\n",
              "      <td>0.621633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C5N</th>\n",
              "      <td>0.587888</td>\n",
              "      <td>0.157137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C6NP</th>\n",
              "      <td>0.710956</td>\n",
              "      <td>0.660519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C6IP</th>\n",
              "      <td>0.857684</td>\n",
              "      <td>0.673481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C6N</th>\n",
              "      <td>0.816264</td>\n",
              "      <td>0.816606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C6A</th>\n",
              "      <td>0.806105</td>\n",
              "      <td>0.806646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C7NP</th>\n",
              "      <td>0.943948</td>\n",
              "      <td>0.561077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C7IP</th>\n",
              "      <td>0.871762</td>\n",
              "      <td>0.587389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C7N</th>\n",
              "      <td>0.930385</td>\n",
              "      <td>1.22368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C7A</th>\n",
              "      <td>0.815116</td>\n",
              "      <td>0.591257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C8NP</th>\n",
              "      <td>0.927734</td>\n",
              "      <td>0.542238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C8IP</th>\n",
              "      <td>0.886431</td>\n",
              "      <td>0.69931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C8N</th>\n",
              "      <td>0.773639</td>\n",
              "      <td>1.1303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C8A</th>\n",
              "      <td>0.658769</td>\n",
              "      <td>0.834661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C9NP</th>\n",
              "      <td>0.923814</td>\n",
              "      <td>0.464974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C9IP</th>\n",
              "      <td>0.877735</td>\n",
              "      <td>0.695504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C9N</th>\n",
              "      <td>0.567576</td>\n",
              "      <td>0.657218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C9A</th>\n",
              "      <td>0.528776</td>\n",
              "      <td>0.62357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C10NP</th>\n",
              "      <td>0.439451</td>\n",
              "      <td>0.366083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C10IP</th>\n",
              "      <td>0.574148</td>\n",
              "      <td>0.866048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C10N</th>\n",
              "      <td>0.259031</td>\n",
              "      <td>0.326697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C10A</th>\n",
              "      <td>0.585694</td>\n",
              "      <td>0.311398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AVG</th>\n",
              "      <td>0.743102</td>\n",
              "      <td>0.648435</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             R2      RMSE\n",
              "C5NP   0.900445  0.696566\n",
              "C5IP    0.84799  0.621633\n",
              "C5N    0.587888  0.157137\n",
              "C6NP   0.710956  0.660519\n",
              "C6IP   0.857684  0.673481\n",
              "C6N    0.816264  0.816606\n",
              "C6A    0.806105  0.806646\n",
              "C7NP   0.943948  0.561077\n",
              "C7IP   0.871762  0.587389\n",
              "C7N    0.930385   1.22368\n",
              "C7A    0.815116  0.591257\n",
              "C8NP   0.927734  0.542238\n",
              "C8IP   0.886431   0.69931\n",
              "C8N    0.773639    1.1303\n",
              "C8A    0.658769  0.834661\n",
              "C9NP   0.923814  0.464974\n",
              "C9IP   0.877735  0.695504\n",
              "C9N    0.567576  0.657218\n",
              "C9A    0.528776   0.62357\n",
              "C10NP  0.439451  0.366083\n",
              "C10IP  0.574148  0.866048\n",
              "C10N   0.259031  0.326697\n",
              "C10A   0.585694  0.311398\n",
              "AVG    0.743102  0.648435"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfS8DEPNp5JF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "457ad81f-56f8-408e-a176-3a0bd0f575da"
      },
      "source": [
        "temp = pd.DataFrame()\n",
        "temp['real'] = y_real[g_cols].sum(axis=1).values\n",
        "temp['pred'] = y_pred[g_cols].sum(axis=1).values\n",
        "temp"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>real</th>\n",
              "      <th>pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>32.206001</td>\n",
              "      <td>32.206001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>38.450001</td>\n",
              "      <td>38.450001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>32.807003</td>\n",
              "      <td>32.806995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>32.024002</td>\n",
              "      <td>32.023998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>51.471004</td>\n",
              "      <td>51.470993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1352</th>\n",
              "      <td>35.746998</td>\n",
              "      <td>35.747002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1353</th>\n",
              "      <td>38.218998</td>\n",
              "      <td>38.218998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1354</th>\n",
              "      <td>45.647999</td>\n",
              "      <td>45.647995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1355</th>\n",
              "      <td>56.240997</td>\n",
              "      <td>56.240997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1356</th>\n",
              "      <td>48.487003</td>\n",
              "      <td>48.487000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1357 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           real       pred\n",
              "0     32.206001  32.206001\n",
              "1     38.450001  38.450001\n",
              "2     32.807003  32.806995\n",
              "3     32.024002  32.023998\n",
              "4     51.471004  51.470993\n",
              "...         ...        ...\n",
              "1352  35.746998  35.747002\n",
              "1353  38.218998  38.218998\n",
              "1354  45.647999  45.647995\n",
              "1355  56.240997  56.240997\n",
              "1356  48.487003  48.487000\n",
              "\n",
              "[1357 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euem1Cb0qALE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}